import os
import csv
import time
import logging
import asyncio
from tqdm import tqdm
import networkx as nx
from pyvis.network import Network
from hf_client import HFClient
from data_agent import keyword_extraction, instruction_judge, field_filter, format_conversion, data_generator_few_shot, data_generator_zero_shot
from config import HUGGINGFACE_TOKEN, TASK_DESCRIPTION, INPUT_FORMAT, OUTPUT_FORMAT

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)

async def hf_data_crawl(task_description, client, task_datasets_count=5, task_data_samples=5):
    # æå–ä»»åŠ¡å…³é”®è¯
    task_keywords = await keyword_extraction(task_description)
    logging.info(f"Extracted Task keywords: {task_keywords}")

    # åˆå§‹åŒ– Hugging Face å®¢æˆ·ç«¯
    client = HFClient(hf_token=HUGGINGFACE_TOKEN)

    dataset_map = {}
    for task_keyword in tqdm(task_keywords, desc="ğŸ” Searching keywords", unit="keyword"):
        # Hugging Face æœç´¢æ•°æ®é›†
        task_datasets = client.search_datasets(task_keyword)
        if not task_datasets:
            continue

        dataset_map[task_keyword] = []
        time.sleep(1)
        
        for task_dataset in tqdm(task_datasets[:task_datasets_count], desc=f"ğŸ“¦ {task_keyword}", leave=False, unit="dataset"):
            # æ‰“å°æ•°æ®é›†åŸºç¡€ä¿¡æ¯
            dataset_id = task_dataset.id
            logging.info(f"Loading dataset: {dataset_id}")
            dataset_info = client.get_info(dataset_id)
            logging.info(f"Loaded dataset info for {dataset_id}")
            dataset_splits = client.get_splits(dataset_id)["splits"]
            logging.info(f"Available splits for {dataset_id}: {dataset_splits}")
            first_split = dataset_splits[0]["split"]
            first_rows = client.get_first_rows(dataset_id, split=first_split)["rows"][:task_data_samples]
            
            # æ•´åˆæ‰€æœ‰çš„å¾…å¤„ç†æ ·æœ¬
            dataset_map[task_keyword].append({
                "id": dataset_id,
                "info": dataset_info,
                "rows": first_rows
            })
            
    return dataset_map
            
async def hf_data_process_sample(row, task_description, input_format, output_format):
    # å•ä¸ªæ ·æœ¬è¿‡æ»¤å’Œå¤„ç†
    row_data = row["row"]
    legal_keys = row_data.keys()
    
    # æ£€æŸ¥å“ªäº›å­—æ®µå¯¹åº”Inputå’ŒOutput
    fields = await field_filter(str(row_data), legal_keys)
    if fields["input"] is None or fields["output"] is None:
        logging.info("Skipping row due to missing input/output fields.")
        return None
    
    # è·å–åŸå§‹çš„Inputå’ŒOutputæ–‡æœ¬
    input_text = row_data.get(fields["input"])
    output_text = row_data.get(fields["output"])
    if input_text is None or output_text is None:
        logging.info("Skipping row due to None input/output values.")
        return None
    
    original_sample = {
        "input": input_text,
        "output": output_text
    }
    
    # å¯¹åŸå§‹æ ·æœ¬è¿›è¡Œä»»åŠ¡é€‚åº”æ€§è¯„åˆ†ï¼Œåªä¿ç•™æ¯ä¸ªåˆ†æ•°å¤§äº8çš„
    sample_scores = await instruction_judge(task_description, str(original_sample))
    for criteria, score in sample_scores.items():
        if int(score) < 8:
            logging.info(f"Skipping row due to low score on {criteria}: {score}")
            return None
        
    # å¯¹ç…§ä»»åŠ¡è¦æ±‚çš„æ ‡å‡†æ ¼å¼è¿›è¡Œè½¬æ¢
    formatted_sample = await format_conversion(
        original_sample["input"],
        original_sample["output"],
        input_format,
        output_format
    )
    if formatted_sample.get("input") is None or formatted_sample.get("output") is None:
        logging.info("Skipping row due to None formatted input/output values.")
        return None
    
    return original_sample, sample_scores, formatted_sample

async def hf_data_process(dataset_map, task_description, input_format, output_format):
    all_tasks = []
    index_map = []

    # æ‰“åŒ…æ‰€æœ‰ä»»åŠ¡
    for keyword, datasets in dataset_map.items():
        for dataset in datasets:
            for row in dataset["rows"]:
                task = asyncio.create_task(hf_data_process_sample(row, task_description, input_format, output_format))
                all_tasks.append(task)
                index_map.append((keyword, dataset["id"], dataset["info"]))

    logging.info(f"ğŸš€ Launching {len(all_tasks)} async sample-processing tasks...")

    # å…¨éƒ¨å¹¶å‘æ‰§è¡Œ
    results = await asyncio.gather(*all_tasks)

    processed_data_map = {}
    for (keyword, dataset_id, dataset_info), result in zip(index_map, results):
        if result is None:
            continue
        original_sample, sample_scores, formatted_sample = result
        if keyword not in processed_data_map:
            processed_data_map[keyword] = []
        # æŸ¥æ‰¾ç°æœ‰ dataset entry æˆ–æ–°å»º
        dataset_entry = next((d for d in processed_data_map[keyword] if d["id"] == dataset_id), None)
        if not dataset_entry:
            dataset_entry = {"id": dataset_id, "info": dataset_info, "samples": []}
            processed_data_map[keyword].append(dataset_entry)
        dataset_entry["samples"].append({
            "original_input": original_sample["input"],
            "original_output": original_sample["output"],
            "scores": sample_scores,
            "formatted_input": formatted_sample["input"],
            "formatted_output": formatted_sample["output"]
        })

    return processed_data_map

def save_data_csv(dataset_map, task_description):
    os.makedirs("hf_logs", exist_ok=True)
    csv_filename = os.path.join("hf_logs", "hf_data_log.csv")
    with open(csv_filename, mode="w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        # å†™å…¥æ ‡é¢˜è¡Œ
        writer.writerow([
            "Task_Definition", "Keyword", "Dataset_ID", 
            "Original_Input", "Original_Output", 
            "Judge_Scores", 
            "Formatted_Input", "Formatted_Output"
        ])

        # éå† dataset_map ç”Ÿæˆæ¯ä¸€è¡Œ
        for kw, datasets in dataset_map.items():
            for dataset in datasets:
                dataset_id = dataset["id"]
                for sample in dataset["samples"]:
                    writer.writerow([
                        task_description, kw, dataset_id,
                        sample["original_input"], sample["original_output"],
                        str(sample["scores"]),
                        sample["formatted_input"], sample["formatted_output"]
                    ])
                    
    logging.info(f"âœ… CSV æ–‡ä»¶å·²ç”Ÿæˆ: {csv_filename}")


if __name__ == "__main__":
    hf_client = HFClient(hf_token=HUGGINGFACE_TOKEN)
    dataset_map = asyncio.run(hf_data_crawl(TASK_DESCRIPTION, hf_client))
    processed_data_map = asyncio.run(
        hf_data_process(dataset_map, TASK_DESCRIPTION, INPUT_FORMAT, OUTPUT_FORMAT)
    )
    save_data_csv(processed_data_map, TASK_DESCRIPTION)