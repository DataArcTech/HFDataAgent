import time
import logging
from tqdm import tqdm
import networkx as nx
from pyvis.network import Network
from hf_client import HFClient
from hf_agent import keyword_extraction, instruction_judge, field_filter, solvable_judge
from config import HUGGINGFACE_TOKEN, TASK_DESCRIPTION

logging.basicConfig(
    level=logging.INFO,  # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º INFO
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)

# -------------------------------
# 1ï¸âƒ£ Hugging Face æ•°æ®é›†æŠ“å–é€»è¾‘
# -------------------------------
client = HFClient(hf_token=HUGGINGFACE_TOKEN)
task_keywords = keyword_extraction(TASK_DESCRIPTION)
print("Extracted Task keywords:", task_keywords)

dataset_map = {}  # {keyword: [ {id, info, rows}, ... ] }

# å¤–å±‚è¿›åº¦æ¡ï¼šå…³é”®è¯çº§åˆ«
for task_keyword in tqdm(task_keywords, desc="ğŸ” Searching keywords", unit="keyword"):
    task_datasets = client.search_datasets(task_keyword)
    if not task_datasets:
        continue

    dataset_map[task_keyword] = []
    time.sleep(1)  # å°å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚

    # å†…å±‚è¿›åº¦æ¡ï¼šæ•°æ®é›†çº§åˆ«
    for task_dataset in tqdm(task_datasets[:3], desc=f"ğŸ“¦ {task_keyword}", leave=False, unit="dataset"):
        dataset_id = task_dataset.id
        logging.info(f"Loading dataset: {dataset_id}")
        dataset_info = client.get_info(dataset_id)
        logging.info(f"Loaded dataset info for {dataset_id}")
        dataset_splits = client.get_splits(dataset_id)["splits"]
        logging.info(f"Available splits for {dataset_id}: {dataset_splits}")
        first_split = dataset_splits[0]["split"]
        first_rows = client.get_first_rows(dataset_id, split=first_split)["rows"][:5]

        samples = []
        for row in first_rows:
            row = row["row"]
            print("Processing row:", row)
            legal_keys = row.keys()
            fields = field_filter(str(row), legal_keys)
            print("Filtered fields:", fields)
            if fields["input"] is None or fields["output"] is None:
                print("Skipping row due to missing input/output fields.")
                continue
            
            input_text = row.get(fields["input"])
            output_text = row.get(fields["output"])
            if input_text is None or output_text is None:
                print("Skipping row due to None input/output values.")
                continue

            sample = {
                "input": input_text,
                "output": output_text
            }            

            sample_score = instruction_judge(TASK_DESCRIPTION, str(sample))
            # sample_solvable = solvable_judge(sample)
            sample["score"] = sample_score
            # sample["solvable"] = sample_solvable
            sample["solvable"] = "Unknown"
            samples.append(sample)
        
        dataset_map[task_keyword].append({
            "id": dataset_id,
            "info": dataset_info,
            "samples": samples
        })


# -------------------------------
# 2ï¸âƒ£ æ„å»ºçŸ¥è¯†å›¾è°±
# -------------------------------
G = nx.Graph()

# æ·»åŠ  task èŠ‚ç‚¹
G.add_node(
    "task",
    label=TASK_DESCRIPTION,
    color="#FFAA00",
    shape="ellipse",
    size=50,  # åŠ å¤§ä¸»ä»»åŠ¡èŠ‚ç‚¹
    physics=True
)

# ä¸ºæ¯ä¸ª keyword æ·»åŠ èŠ‚ç‚¹å¹¶è¿æ¥ task
for kw in task_keywords:
    kw_node = f"kw::{kw}"
    G.add_node(
        kw_node,
        label=kw,
        color="#00CCFF",
        shape="box",
        size=30  # å…³é”®å­—èŠ‚ç‚¹ç¨å¤§
    )
    G.add_edge("task", kw_node)

    # è‹¥è¯¥ keyword æœ‰åŒ¹é…åˆ°æ•°æ®é›†
    if kw in dataset_map:
        for dataset in dataset_map[kw]:
            dataset_id = dataset["id"]
            dataset_info = dataset["info"]

            # æ·»åŠ  dataset èŠ‚ç‚¹
            ds_node = f"ds::{dataset_id}"
            G.add_node(
                ds_node,
                label=dataset_id,
                color="#33FF66",
                title=f"Info: {dataset_info}",
                size=20
            )
            G.add_edge(kw_node, ds_node)

            # æ·»åŠ  dataset samples èŠ‚ç‚¹
            for i, sample in enumerate(dataset["samples"]):
                sample_node = f"sample::{dataset_id}::{i}"
                sample_preview = str(sample)[:200]
                G.add_node(
                    sample_node,
                    label=f"Sample {i+1}",
                    title=sample_preview,
                    color="#AAAAAA",
                    size=8
                )
                G.add_edge(ds_node, sample_node)


# -------------------------------
# 3ï¸âƒ£ ç»˜åˆ¶äº¤äº’å¼å›¾è°±
# -------------------------------
net = Network(
    height="1600px",
    width="100%",
    bgcolor="#222222",
    font_color="white"
)

net.from_nx(G)
net.force_atlas_2based()
net.show("dataset_graph.html", notebook=False)

print("âœ… å›¾è°±å·²ç”Ÿæˆ: dataset_graph.html")


import csv

# -------------------------------
# 4ï¸âƒ£ å¯¼å‡ºç»“æœåˆ° CSV
# -------------------------------
csv_filename = "dataset_summary.csv"

with open(csv_filename, mode="w", encoding="utf-8", newline="") as f:
    writer = csv.writer(f)
    # å†™å…¥æ ‡é¢˜è¡Œ
    writer.writerow(["Task_Definition", "Keyword", "Dataset_ID", "Sample_Input", "Sample_Output", "Judge_Score", "Judge_Solvable"])

    # éå† dataset_map ç”Ÿæˆæ¯ä¸€è¡Œ
    for kw, datasets in dataset_map.items():
        for dataset in datasets:
            dataset_id = dataset["id"]
            for sample in dataset["samples"]:
                writer.writerow([TASK_DESCRIPTION, kw, dataset_id, sample["input"], sample["output"], sample["score"], sample["solvable"]])

print(f"âœ… CSV æ–‡ä»¶å·²ç”Ÿæˆ: {csv_filename}")